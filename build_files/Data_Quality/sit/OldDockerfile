e an official Ubuntu as a parent image
FROM ubuntu:20.04
FROM node:16.16.0
FROM openjdk:8-jdk

WORKDIR /home/hadoop
COPY ./api ./DataQuality/api/
COPY ./ui ./DataQuality/ui/

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV JAVA_HOME=/home/hadoop/java
ENV HADOOP_HOME=/home/hadoop/hadoop
ENV HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop
ENV HADOOP_MAPRED_HOME=/home/hadoop/hadoop
ENV HADOOP_COMMON_HOME=/home/hadoop/hadoop
ENV HADOOP_HDFS_HOME=/home/hadoop/hadoop
ENV YARN_HOME=/home/hadoop/hadoop
ENV PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
ENV HADOOP_COMMON_LIB_NATIVE_DIR=/home/hadoop/hadoop/lib/native
ENV HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=/home/hadoop/hadoop/lib"
ENV SPARK_HOME=/home/hadoop/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV PYSPARK_SUBMIT_ARGS="--master local[3] pyspark-shell"
ENV HIVE_HOME=/home/hadoop/hive
ENV PATH=$PATH:$HIVE_HOME/bin
ENV HADOOP_PID_DIR=/home/hadoop/hadoop/data/hdfs/pid

# Install required packages
RUN apt-get update && apt-get install -y \
    apt-utils \
    wget \
    tar \
    vim \
    lsof \
    openssh-server \
    sshpass \
    python3 \
    python3-pip \
    rsync \
    curl \
    gnupg \
    software-properties-common \
    nodejs \
    npm \
    && rm -rf /var/lib/apt/lists/*

# Install Node.js and npm (version 16.x)
RUN curl -fsSL https://deb.nodesource.com/setup_16.x | bash - \
    && apt-get install -y nodejs \
    && npm install -g @angular/cli@13.1.2

# Download Hadoop, Hive, Java and Spark
WORKDIR /home/hadoop

RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz \
    && wget https://archive.apache.org/dist/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz \
    && wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz \
    && wget https://download.oracle.com/java/17/latest/jdk-17_linux-x64_bin.tar.gz  \
    && tar -xvf hadoop-2.7.2.tar.gz \
    && tar -xvf apache-hive-1.2.2-bin.tar.gz \
    && tar -xvf spark-3.5.0-bin-hadoop3.tgz \
    && tar -xvzf jdk-17_linux-x64_bin.tar.gz \
    && rm hadoop-2.7.2.tar.gz apache-hive-1.2.2-bin.tar.gz spark-3.5.0-bin-hadoop3.tgz jdk-17_linux-x64_bin.tar.gz

RUN mv hadoop-2.7.2 hadoop \
    && mv spark-3.5.0-bin-hadoop3 spark \
    && mv apache-hive-1.2.2-bin hive \
    && mv jdk-17.0.12 java

# Configure Hadoop
RUN cat /home/hadoop/DataQuality/api/dq_env/conf/hadoop/core-site.xml > $HADOOP_CONF_DIR/core-site.xml
RUN cat /home/hadoop/DataQuality/api/dq_env/conf/hadoop/hdfs-site.xml > $HADOOP_CONF_DIR/hdfs-site.xml
RUN cat /home/hadoop/DataQuality/api/dq_env/conf/hadoop/mapred-site.xml > $HADOOP_CONF_DIR/mapred-site.xml
RUN cat /home/hadoop/DataQuality/api/dq_env/conf/hadoop/yarn-site.xml > $HADOOP_CONF_DIR/yarn-site.xml
RUN cat /home/hadoop/DataQuality/api/dq_env/conf/hadoop/hadoop-env.sh > $HADOOP_CONF_DIR/hadoop-env.sh
RUN sed -i 's|${HADOOP_HOME}|/home/hadoop/hadoop|g' $HIVE_HOME/bin/hive-config.sh

# Set up SSH
RUN mkdir /var/run/sshd \
    && echo 'root:hadoop' | chpasswd \
    && ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa \
    && cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys \
    && echo 'StrictHostKeyChecking no' >> /etc/ssh/ssh_config

# Format HDFS and start services
RUN mkdir -p /home/hadoop/hadoop/data/hdfs/namenode \
    && mkdir -p /home/hadoop/hadoop/data/hdfs/datanode \
    && /home/hadoop/hadoop/bin/hdfs namenode -format


RUN chmod +x /home/hadoop/DataQuality/api/dq_env/start-all.sh

# Expose ports
EXPOSE 22 9000 4900 3338 7859 27017

# Start all services
CMD ["/home/hadoop/DataQuality/api/dq_env/start-all.sh"]
	
